
@article{bell2007,
  title = {The {{BellKor}} Solution to the {{Netflix Prize}}},
  author = {Bell, Robert M and Koren, Yehuda and Volinsky, Chris and Labs, T},
  year = {2007},
  pages = {15},
  file = {C\:\\Users\\p_m_g\\Zotero\\storage\\2MDWGQW8\\Bell et al. - The BellKor solution to the Netflix Prize.pdf},
  language = {en}
}

@misc{chen2011,
  title = {Winning the {{Netflix Prize}}: {{A Summary}}},
  author = {Chen, Edward},
  year = {2011},
  month = oct,
  file = {C\:\\Users\\p_m_g\\Zotero\\storage\\Z5ZMEYEX\\winning-the-netflix-prize-a-summary.html},
  howpublished = {http://blog.echen.me/2011/10/24/winning-the-netflix-prize-a-summary/}
}

@article{chin2015,
  title = {A {{Fast Parallel Stochastic Gradient Method}} for {{Matrix Factorization}} in {{Shared Memory Systems}}},
  author = {Chin, Wei-Sheng and Zhuang, Yong and Juan, Yu-Chin and Lin, Chih-Jen},
  year = {2015},
  month = mar,
  volume = {6},
  pages = {1--24},
  issn = {2157-6904, 2157-6912},
  doi = {10.1145/2668133},
  file = {C\:\\Users\\p_m_g\\Zotero\\storage\\YM8PZSIU\\Chin et al. - 2015 - A Fast Parallel Stochastic Gradient Method for Mat.pdf},
  journal = {ACM Trans. Intell. Syst. Technol.},
  language = {en},
  number = {1}
}

@incollection{chin2015a,
  title = {A {{Learning}}-{{Rate Schedule}} for {{Stochastic Gradient Methods}} to {{Matrix Factorization}}},
  booktitle = {Advances in {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Chin, Wei-Sheng and Zhuang, Yong and Juan, Yu-Chin and Lin, Chih-Jen},
  editor = {Cao, Tru and Lim, Ee-Peng and Zhou, Zhi-Hua and Ho, Tu-Bao and Cheung, David and Motoda, Hiroshi},
  year = {2015},
  volume = {9077},
  pages = {442--455},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-18038-0_35},
  abstract = {Stochastic gradient methods are effective to solve matrix factorization problems. However, it is well known that the performance of stochastic gradient method highly depends on the learning rate schedule used; a good schedule can significantly boost the training process. In this paper, motivated from past works on convex optimization which assign a learning rate for each variable, we propose a new schedule for matrix factorization. The experiments demonstrate that the proposed schedule leads to faster convergence than existing ones. Our schedule uses the same parameter on all data sets included in our experiments; that is, the time spent on learning rate selection can be significantly reduced. By applying this schedule to a state-of-the-art matrix factorization package, the resulting implementation outperforms available parallel matrix factorization packages.},
  file = {C\:\\Users\\p_m_g\\Zotero\\storage\\WAT7W2HE\\Chin et al. - 2015 - A Learning-Rate Schedule for Stochastic Gradient M.pdf},
  isbn = {978-3-319-18037-3 978-3-319-18038-0},
  language = {en}
}

@article{china,
  title = {{{LIBMF}}: {{A Library}} for {{Parallel Matrix Factorization}} in {{Shared}}-Memory {{Systems}}},
  author = {Chin, Wei-Sheng and Yuan, Bo-Wen and Yang, Meng-Yuan and Zhuang, Yong and Juan, Yu-Chin and Lin, Chih-Jen},
  pages = {5},
  abstract = {Matrix factorization (MF) plays a key role in many applications such as recommender systems and computer vision, but MF may take long running time for handling large matrices commonly seen in the big data era. Many parallel techniques have been proposed to reduce the running time, but few parallel MF packages are available. Therefore, we present an open source library, LIBMF, based on recent advances of parallel MF for sharedmemory systems. LIBMF includes easy-to-use command-line tools, interfaces to C/C++ languages, and comprehensive documentation. Our experiments demonstrate that LIBMF outperforms state of the art packages. LIBMF is BSD-licensed, so users can freely use, modify, and redistribute the code.},
  file = {C\:\\Users\\p_m_g\\Zotero\\storage\\EGAISHVH\\Chin et al. - LIBMF A Library for Parallel Matrix Factorization.pdf},
  journal = {MEMORY SYSTEMS},
  language = {en}
}

@misc{funk2006,
  title = {Netflix {{Update}}: {{Try This}} at {{Home}}},
  author = {Funk, Simon},
  year = {2006},
  month = dec,
  file = {C\:\\Users\\p_m_g\\Zotero\\storage\\HJDEUBB7\\20061211.html},
  howpublished = {https://sifter.org/\textasciitilde simon/journal/20061211.html}
}

@misc{grouplens2013,
  title = {What Is {{GroupLens}}?},
  author = {GroupLens},
  year = {2013},
  month = sep,
  abstract = {GroupLens is a research lab in the Department of Computer Science and Engineering at the University of Minnesota, Twin Cities specializing in recommender systems, online communities, mobile and ubi\ldots},
  file = {C\:\\Users\\p_m_g\\Zotero\\storage\\K53N5JBX\\what-is-grouplens.html},
  howpublished = {https://grouplens.org/about/what-is-grouplens/},
  journal = {GroupLens},
  language = {en}
}

@book{irizarry2021,
  title = {Introduction to {{Data Science}}},
  author = {Irizarry, Rafael A.},
  year = {2021},
  abstract = {This book introduces concepts and skills that can help you tackle real-world data analysis challenges. It covers concepts from probability, statistical inference, linear regression and machine learning and helps you develop skills such as R programming, data wrangling with dplyr, data visualization with ggplot2, file organization with UNIX/Linux shell, version control with GitHub, and reproducible document preparation with R markdown.},
  file = {C\:\\Users\\p_m_g\\Zotero\\storage\\X8WBQU2X\\dsbook.html}
}

@book{irizarry2021a,
  title = {Chapter 33 {{Large}} Datasets | {{Introduction}} to {{Data Science}}},
  author = {Irizarry, Rafael A.},
  year = {2021},
  abstract = {This book introduces concepts and skills that can help you tackle real-world data analysis challenges. It covers concepts from probability, statistical inference, linear regression and machine learning and helps you develop skills such as R programming, data wrangling with dplyr, data visualization with ggplot2, file organization with UNIX/Linux shell, version control with GitHub, and reproducible document preparation with R markdown.},
  file = {C\:\\Users\\p_m_g\\Zotero\\storage\\CRHB2HRJ\\large-datasets.html}
}

@article{koren2009,
  title = {The {{BellKor Solution}} to the {{Netflix Grand Prize}}},
  author = {Koren, Yehuda},
  year = {2009},
  pages = {10},
  file = {C\:\\Users\\p_m_g\\Zotero\\storage\\NGCN8YUS\\Koren - The BellKor Solution to the Netï¬‚ix Grand Prize.pdf},
  language = {en}
}

@misc{qiu2021,
  title = {Yixuan/Recosystem},
  author = {Qiu, Yixuan},
  year = {2021},
  month = jan,
  abstract = {Recommender System Using Parallel Matrix Factorization},
  copyright = {View license         ,                 View license},
  keywords = {matrix-factorization,recommender-system}
}

@article{rendle2019,
  title = {On the {{Difficulty}} of {{Evaluating Baselines}}: {{A Study}} on {{Recommender Systems}}},
  shorttitle = {On the {{Difficulty}} of {{Evaluating Baselines}}},
  author = {Rendle, Steffen and Zhang, Li and Koren, Yehuda},
  year = {2019},
  month = may,
  abstract = {Numerical evaluations with comparisons to baselines play a central role when judging research in recommender systems. In this paper, we show that running baselines properly is difficult. We demonstrate this issue on two extensively studied datasets. First, we show that results for baselines that have been used in numerous publications over the past five years for the Movielens 10M benchmark are suboptimal. With a careful setup of a vanilla matrix factorization baseline, we are not only able to improve upon the reported results for this baseline but even outperform the reported results of any newly proposed method. Secondly, we recap the tremendous effort that was required by the community to obtain high quality results for simple methods on the Netflix Prize. Our results indicate that empirical findings in research papers are questionable unless they were obtained on standardized benchmarks where baselines have been tuned extensively by the research community.},
  archiveprefix = {arXiv},
  eprint = {1905.01395},
  eprinttype = {arxiv},
  file = {C\:\\Users\\p_m_g\\Zotero\\storage\\9DR97T6U\\Rendle et al. - 2019 - On the Difficulty of Evaluating Baselines A Study.pdf;C\:\\Users\\p_m_g\\Zotero\\storage\\CB7TJ8DZ\\1905.html},
  journal = {arXiv:1905.01395 [cs]},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning},
  primaryclass = {cs}
}


